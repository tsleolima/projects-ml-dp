{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012d1a4b",
   "metadata": {},
   "source": [
    "### Vetorização de texto\n",
    "\n",
    "É uma representação do texto em **vetores** para que seja permitida sua aplicabilidade em modelos de aprendizado de máquina.\n",
    "\n",
    "![Vector Space](assets/vector_space.png \"Vector Space\").\n",
    "\n",
    "**Obs:** Antes de tudo, é importante entender que os termos podem ser vistos como gramas, e a qualquer momento podemos avaliar a vetorização como não apenas uma palavra única, e sim como uma sequência, por exemplo 2 palavras por token ou 2-grama.\n",
    "\n",
    "Exemplo: \"Ola estou bem\" \n",
    "- 1-grama: [\"ola\", \"estou\", \"bem\"]\n",
    "- 2-grama: [\"ola estou\", \"estou bem\"]\n",
    "\n",
    "Nos exemplos abaixo iremos usar apenas 1-grama.\n",
    "\n",
    "Na figura acima podemos observar que na vetorização cada termo é mais uma **dimensão** no espaço vetorial, isso indica que essas dimensões tem direções independentes e ortogonais entre si.\n",
    "\n",
    "Quando vetorizamos um corpus real o espaço dimensional é gigantesco dado o grande número de palavras.\n",
    "\n",
    "As sentenças podem ser visualizadas como vetores que possem direção, módulo e sentido. Esses valores são correspondestes as dimensões do espaço vetorial **por exemplo**, a sentença1={term_1, term_2, term_n}.\n",
    "\n",
    "Dado que um documento pode ser formado por diversos termos, isso quer dizer que essa sentença/vetor é composta por valores referentes a todas as dimensões ou palavras do corpus. Logo, pode ser visto como uma maneira de reconhecer um certo nível de entendimento da sentença.\n",
    "\n",
    "O Nosso desafio é como obteremos esses valores que compõem um documento/sentença, para isso temos as seguintes formas de lidar com isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe27eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"the red dog\",\n",
    "    \"cat eats dog\",\n",
    "    \"dog eats food\",\n",
    "    \"red cat eats\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2758502",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e33e1",
   "metadata": {},
   "source": [
    "**Bag of words (BOW)**: Como o nome diz, é um saco de palavras. Iremos usar 1 se a palavra estive presente no documento e 0 se ela não estiver presente. \n",
    "\n",
    "![Bag Of Words](assets/bow.png \"Bag Of Words\").\n",
    "\n",
    "Por exemplo, na frase \"The red dog\" teremos o valor 1 para os termos \"THE\", \"RED\" e \"DOG\". \n",
    "Iremos realizar o procedimento para todos os documentos do nosso corpus.\n",
    "\n",
    "Mas... porque chamamos isso de **saco** de palavras ? Isso se dá ao fato de perdemos informações relacionadas a ordem/sequência das palavras, transformando apenas em um conjunnto, garantindo apenas se o termo existe ou não no documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279cd1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cat': 0, 'dog': 1, 'eats': 0, 'food': 0, 'red': 1, 'the': 1},\n",
       " {'cat': 1, 'dog': 1, 'eats': 1, 'food': 0, 'red': 0, 'the': 0},\n",
       " {'cat': 0, 'dog': 1, 'eats': 1, 'food': 1, 'red': 0, 'the': 0},\n",
       " {'cat': 1, 'dog': 0, 'eats': 1, 'food': 0, 'red': 1, 'the': 0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcule_bow(sentences):\n",
    "    listOfList = [sentence.split() for sentence in corpus] # Obtem lista de lista de termos\n",
    "    word_set = set().union(*listOfList) # Obtem as palavras unicas do corpus\n",
    "    \n",
    "    word_in = dict.fromkeys(word_set,0) # transforma em um dict com chave=palavra e value=0\n",
    "    bow = [] # lista que vai conter todo o corpus vetorizado\n",
    "    for sentence in sentences: # Iterando nas sentencas\n",
    "        for word in sentence.split(): # Iterando nos termos\n",
    "            word_in[word] = 1 # Colocando 1 na posicao do termo da sentenca\n",
    "\n",
    "        bow.append(word_in) # salvando na lista\n",
    "        word_in = dict.fromkeys(word_set,0) # resetando o dict\n",
    "        \n",
    "    return bow \n",
    "\n",
    "calcule_bow(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbfdf96",
   "metadata": {},
   "source": [
    "Já com essa abordagem inicial podemos tentar calcular similaridades em as sentenças usando um simples **produto escalar**.\n",
    "\n",
    "$$\\vec{p}\\cdot\\vec{q}=x_{1}x_{2}+y_{1}y_{2}+z_{n}z_{n}$$\n",
    "\n",
    "Vale lembrar que no produto escalar, quando temos vetores ortogonais a similaridade é 0, portanto as sentenças **não** teriam similaridades. (Se quiser ver com mais detalhes, adicione dois vetores e multiplique eles no https://www.geogebra.org/classic)\n",
    "\n",
    "Uma curiosidade é que no Bag of words quando realizamos o produto escalar podemos encontrar a **intersecção** entre as duas sentenças que estamos observando, vamos ver um exemplo:\n",
    "\n",
    "Se usarmos as frases: \"The red dog\" e \"cat eats dog\" temos os seguintes vetores\n",
    "\n",
    "A = \"The red dog\": [0, 1, 0, 0 , 1, 1]\n",
    "B = \"cat eats dog\": [1, 1, 1, 0, 0, 0]\n",
    "\n",
    "A * B = (0 * 1) + (1 * 1) + (0 * 1) + (0 * 0) + (1 * 0) + (1 * 0)\n",
    "\n",
    "A **única** palavra que é igual nas duas frases é **dog**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507adaea",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22514b",
   "metadata": {},
   "source": [
    "**Term Frequency (TF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee29ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
